# 教工委新闻自动化简报系统 - 产品需求文档（PRD）

## 1. 背景与目标
目前教工委新闻简报依赖人工整理，过程繁琐、重复度高，且容易遗漏。  
目标是开发一个自动化系统，实现新闻的**采集、筛选、聚合、摘要、排序、人工校验与简报生成**，提高效率和准确性。

---

## 2. 功能需求

### 2.1 自动爬取新闻
- **来源**：
  - 今日头条（指定媒体全部新闻）
  - 部分指定网站
  - 搜索引擎（按关键词抓取当日新增新闻）
- **输出**：原始新闻列表（含标题、来源、正文、发布时间、链接）

### 2.2 过滤与去重
- **去重**：
  - 标题相同或正文高相似度 → 保留一条
- **教育相关性筛选**：
  - 使用教育关键词列表，若新闻未命中任何关键词 → 丢弃
- **输出**：初筛新闻列表

### 2.3 新闻溯源与聚合
- **溯源规则**：
  - 找到最近的**官方媒体报道**
  - 若源头为自媒体/个人账号 → 截止在最近的官方媒体
- **聚合规则**：
  - 将多篇报道聚合为一个事件集合
- **输出**：带溯源链路的新闻集合

### 2.4 摘要与重点提取
- **摘要要求**：
  - 不仅是简要概括，还要标注与**教工委关注点**相关的内容
  - 示例：活动、政策、校园开放、师生安全等
- **输出**：带重点摘要的新闻集合

### 2.5 新闻排序
- **打分维度**：
  - 新闻重要性：涉及范围大、社会关注度高
  - 来源权威性：新华社/人民日报/央视等加权
  - 教工委相关性：与活动、政策、关切话题强相关加权
- **输出**：系统按分数排序的新闻列表

### 2.6 人工选择
- 提供交互界面，人工点选需要的新闻进入简报生成环节

### 2.7 简报生成
- **排序规则**：中小学 → 高校 → 其他社会新闻
- **格式**：纯文本（可直接复制）
- **内容**：
  - 日期 + 报告期号
  - 按分类输出新闻标题 + 摘要 + 来源
- **输出**：当日新闻简报（TXT）

### 2.8 入库管理（PostgreSQL）
- 所有已经爬下来的新闻均保存（无论是否被筛选），但最终被提交的新闻需要能够区分出来
- 反馈更新：
  - 相同新闻的修改 → 覆盖（通常可以按照标题来匹配，但有些时候标题也会被修改，可能需要向量相似度匹配，此时可以弹窗让人工确认一下）
  - 新新闻 → 新增

---

## 3. 非功能需求
- **稳定性**：每天定时运行，容错能力（网络错误自动重试）
- **可扩展性**：可新增关键词和媒体来源，配置文件驱动
- **易维护性**：日志与错误提示清晰，支持快速排查

---

## 4. 系统流程图（示意）

```
爬取新闻 → 去重筛选 → 溯源聚合 → 摘要提取 → 系统排序 → 人工选择 → 简报生成 → 入库
```

---

## 5. 优先级

- **MVP（最小可用版本）**  
  - 新闻爬取  
  - 去重筛选  
  - 摘要生成  
  - 人工选择  
  - 简报输出  

- **迭代功能**  
  - 溯源聚合  
  - 自动打分排序  
  - 入库反馈覆盖更新  

---

## 6. 未来扩展
- 支持多渠道输出（Word/PDF/邮件推送）
- 数据可视化（新闻热度、关键词趋势）
- 引入机器学习模型改进去重与教育相关性判断

---

## 7. 技术实现路线（Supabase）

### 7.1 系统架构概览
- **数据采集层**：爬虫服务（Python）按计划任务运行，使用消息队列（如Supabase Edge Functions触发或Cron服务）将新闻写入Supabase PostgreSQL。
- **处理服务层**：独立的ETL/分析服务（可部署为Serverless或容器），负责去重、关键词筛选、溯源聚合、摘要生成等步骤，处理后的结果同步回Supabase。
- **应用层**：后台管理界面（例如Next.js + Supabase Auth），提供人工审核、编辑与简报生成；未来可扩展到多渠道发布。
- **监控与日志**：利用Supabase Logflare或外部服务（如Sentry）记录爬虫与处理流程状态。

### 7.2 数据库设计（Supabase PostgreSQL）
| 表名 | 说明 | 核心字段 |
| --- | --- | --- |
| `sources` | 新闻来源配置 | `id`, `name`, `type`, `base_url`, `priority`, `is_active`, `metadata` |
| `raw_articles` | 原始抓取新闻 | `id`, `source_id`, `title`, `content`, `published_at`, `url`, `raw_payload`, `hash`, `created_at` |
| `filtered_articles` | 初筛后的新闻快照 | `id`, `raw_article_id`, `relevance_score`, `status`, `keywords`, `dedup_group_id`, `processed_payload` |
| `events` | 聚合后的新闻事件 | `id`, `title`, `summary`, `importance_score`, `source_level`, `primary_source_url`, `status`, `created_at`, `updated_at` |
| `event_articles` | 事件与文章映射 | `id`, `event_id`, `filtered_article_id`, `role`, `notes` |
| `brief_items` | 简报条目 | `id`, `event_id`, `section`, `order_index`, `final_summary`, `approved_by`, `approved_at` |
| `brief_batches` | 每日报告 | `id`, `report_date`, `sequence_no`, `generated_at`, `generated_by`, `export_payload` |
| `audit_logs` | 操作与反馈 | `id`, `entity_type`, `entity_id`, `action`, `payload`, `created_by`, `created_at` |

> 说明：`raw_articles` 与 `filtered_articles` 采用软删除字段以保留历史；`events`、`brief_items` 支持人工修改并通过 `audit_logs` 保持追踪。

### 7.3 迭代计划
1. **Sprint 1：基础数据链路（1-2周）**
   - 搭建Supabase项目，创建上述核心数据表及存储桶。
   - 完成至少一个新闻来源（今日头条）的爬虫，写入 `raw_articles`。
   - 实现关键词过滤与去重逻辑，产出 `filtered_articles`。
   - 编写基础运维脚本：任务调度、失败重试、日志记录。

2. **Sprint 2：摘要与简报MVP（2周）**
   - 集成LLM或摘要算法，为 `filtered_articles` 生成摘要与重点字段。
   - 建立初版 `events` 聚合逻辑（简单按标题/向量相似度分组）。
   - 开发简易前端界面：列表查看、人工选择新闻生成 `brief_items`。
   - 支持导出TXT格式简报，并记录到 `brief_batches`。

3. **Sprint 3：增强与监控（2周）**
   - 完善溯源链路，优先显示官方媒体；增加评分模型字段。
   - 引入Supabase Auth，区分管理员与编辑角色，保护后台访问。
   - 增加审核日志与反馈通道，建立 `audit_logs` 写入。
   - 建立关键指标监控（每日抓取量、过滤率、人工操作量）。

4. **后续迭代方向**
   - 拓展多来源爬虫与自定义关键词配置，支持动态更新。
   - 实现自动排序模型（基于权重或ML），优化 `importance_score`。
   - 推出多渠道输出（邮件/Word/PDF），并在Supabase存储对应文件。
   - 引入数据可视化仪表盘，展示趋势与统计数据。

### 7.4 工程与协作建议
- 建立基础CI流程（Lint + 单元测试），保证数据处理脚本质量。
- 使用Supabase的Row Level Security与Policies，确保数据访问安全。
- 制定数据字典与API文档，方便团队协作与后续维护。
- 每个Sprint结束进行回顾，收集人工审核的痛点，持续优化流程。
