▌ 请阅读D:\2program_practise\202509news-article中的脚本，这是我目前的项目情况，我希望在这个基础上开发，把数据改换到
▌ supabase上运行，请仔细思考接下来应该如何操作

> 现状梳理

  - run_pipeline.py:19 以外部脚本方式串联导入、回填、摘要、打分、导出和清理步骤，每一步都假定核心数据来自本地
  articles.sqlite3。
  - tools/import_authorfetch_to_sqlite.py:40 创建并维护 articles 表，字段与今日头条抓取内容紧耦合；upsert_article_row 在
  本地重复利用 article_id 做幂等写入。
  - tools/summarize_news.py:313 直接对 articles 左连接 news_summaries 判定待处理文章；摘要结果同样写入 SQLite 的
  news_summaries 表并在本地更新关联字段。
  - tools/score_correlation_fulltext.py:63 为 news_summaries 增列 correlation，随后批量更新分值。
  - tools/export_high_correlation.py:74 依赖本地表 export_history 做导出去重及记录。

  迁移目标映射

  - Supabase schema 已在 edu_news_pipeline/supabase/schema.sql:1 创建：sources、raw_articles、filtered_articles、
  brief_batches / brief_items 等。
  - 建议映射：本地 articles → raw_articles（hash 用 article_id + URL 生成；metadata 存原始 Excel 信息）；news_summaries
  → filtered_articles（summary、relevance_score、processed_payload）；export_history → brief_batches/brief_items（批次记
  录与明细）。

  实施步骤（推荐顺序）

  1. 抽象数据访问层
      - 新建模块（例如 services/db_adapter.py）统一封装当前 SQLite 查询写入接口；让现有脚本通过该适配器工作，保持功能不
  变，降低后续替换风险。
  2. 实现 Supabase 适配器
      - 基于 supabase Python SDK 创建 SupabaseRepository，封装：获取或新建 sources、按 hash upsert raw_articles、查询待
  摘要文章（联表 filtered_articles）、写入或更新 filtered_articles、创建导出批次（brief_batches+brief_items）。
      - 处理数据类型转换：publish_time 统一转换为 timestamptz；keywords 写入 filtered_articles.keywords；全文内容放在
  processed_payload 或保留在 raw_articles.content。
  3. 逐步替换脚本逻辑
      - 修改 tools/import_authorfetch_to_sqlite.py（可更名为 import_authorfetch.py）调用 Supabase 适配器写入；保留原
  SQLite 版本以备回滚。
      - 更新 tools/summarize_news.py：通过适配器批量拉取未处理 raw_articles，调用 LLM 后把摘要写入 filtered_articles（含
  raw_article_id、summary、source、keywords）；按需写 processed_payload 保存原文。
      - 更新 tools/score_correlation_fulltext.py：改为检索 filtered_articles 中 relevance_score IS NULL 记录，写回
  relevance_score。
      - 更新 tools/export_high_correlation.py：查询 filtered_articles + raw_articles 生成导出文本；导出后创建或复用
  brief_batches 并在 brief_items 中写入 final_summary、section（由原分类逻辑决定）、order_index；skip-exported 逻辑改为
  检查同批次的 brief_items 是否已有该 filtered_article_id。
      - run_pipeline.py 仅需更新默认配置（如改传 Supabase 模式标识）和 CLI 参数，核心流水线不变。
  4. 配置与凭据管理
      - 在本仓库新增 .env.example（含 SUPABASE_URL、SUPABASE_ANON_KEY、SUPABASE_DB_PASSWORD）；加载逻辑复用 tools/
  summarize_news.py 的 load_dotenv_simple。
      - .gitignore 增加 .env.local 等敏感文件规则。
  5. 数据迁移脚本
      - 编写一次性脚本：读取 articles.sqlite3，按上述映射批量写入 Supabase；同步历史摘要和导出记录，必要时写
  processed_payload 存原始行，确保线上环境接力本地成果。
  6. 验证与运行
      - 为关键接口加集成测试（tests/ 目录）——使用 Supabase 的 REST mock 或本地回放；重点验证 upsert 幂等性与导出批次
  写入。
      - 在测试环境运行 run_pipeline.py --skip-export ... 确认各阶段可用，再开放生产运行。

  注意事项

  - Supabase 匿名 Key 权限有限：写入 raw_articles/filtered_articles 可能需服务角色或 RLS 策略；在 Supabase 控制台为操作
  脚本绑定专用服务 key，并通过环境变量管理。
  - 并发与速率：Supabase REST API 有速率限制，需在进口/摘要阶段加限流与重试；可利用批量 upsert（insert(...,
  returning='representation', count='exact')）减少往返。
  - RLS 策略：迁移后务必在 Supabase 启用 Row Level Security 并配置策略（示例已在 schema.sql 注释），避免公开 anon key
  可写。
  - 导出分类：brief_items.section 目前枚举为 primary_school、high_school 等，可与 CATEGORY_ORDER 映射表对齐，必要时调整
  schema 或增加映射层。

  按照以上步骤，可先保持本地 SQLite 流程可运行，再平滑切换至 Supabase，实现云端存储与后续联动扩展。